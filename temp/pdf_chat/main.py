import os
import streamlit as st
from streamlit_chat import message
# pdf loader + split
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
# docs embedding and vectorstore
# from langchain.embeddings import OpenAIEmbeddings
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Milvus 
# retriever and reranking
from langchain.retrievers import MilvusRetriever
# LLM ì—°ê²°, QA chain êµ¬ì„±
from langchain.llms import OpenAI
from langchain.llms import LlamaCpp
from langchain.chains import RetrievalQA

st.title("PDF Chatbot for Data Drift")
st.write("Upload a PDF file to chat with the content.")

# initialize session state
if 'history' not in st.session_state:
    st.session_state['history'] = []

# upload PDF
uploaded_file = st.file_uploader(" ", type=["pdf", "txt", "docx"])

if uploaded_file is not None:
    # íŒŒì¼ í¬ê¸° ì œí•œ 
    max_size_mb = 100
    if uploaded_file.size > max_size_mb * 1024 * 1024:
        st.error(f"File size exceeds {max_size_mb}MB limit.")
        st.stop()

    st.write("ğŸ˜€ File uploaded successfully!")

    # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
    safe_filename = os.path.basename(uploaded_file.name).replace(" ", "_")
    temp_file_path = os.path.join("temp", safe_filename)
    os.makedirs("temp", exist_ok=True)
    with open(temp_file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.write("ğŸ’¾ File saved to temp directory.")

    # show a loading spinner while processing the file
    with st.spinner("Processing..."):
        # ë¶„í• 
        loader = PyPDFLoader(temp_file_path)
        documents = loader.load()
        txt_split = RecursiveCharacterTextSplitter(  # Split the documents into chunks
            chunk_size=1000,
            chunk_overlap=200,
        )
        texts = txt_split.split_documents(documents)
        st.write(f"âœ”ï¸ Loaded {len(texts)} chunks from the PDF file.")

        # ë²¡í„° ì„ë² ë”© ; Create embeddings and vectorstore
        embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = Milvus.from_documents(texts, embedding_model, connection_args={"host": "localhost", "port": "19530"})
        st.write("Embeddings and vectorstore created.")

        # ì¬ê²€ìƒ‰ê¸° ì„¤ì • ; reranker ì—†ì´ ê¸°ë³¸ retrieverë§Œ ì‚¬ìš©
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        # ë­ì²´ì¸ ì†Œí™˜ ; LangChainìš© LlamaCpp ëª¨ë¸ ì„¤ì •
        llm = LlamaCpp(
            # Path to the model file ; gguf í¬ë§· ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì„œ ì§ì ‘ ê²½ë¡œ ì§€ì •
            model_path="/home/keti/datadrift_jm/models/gpt4all/ggml-model-Q4_K_M.gguf",
            temperature=0.7,
            max_tokens=512,
            top_p=0.9,
            n_ctx=2048,
            n_batch=128,        # ìƒì„± ì†ë„ í–¥ìƒ (VRAM ìƒí™©ì— ë”°ë¼ ì¡°ì •)
            n_threads=8,        # CPU thread ìˆ˜ (ë©€í‹°ì½”ì–´ ì‚¬ìš© ì‹œ)
            n_gpu_layers=-1,    # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUë¡œ ì˜¬ë¦¼
            verbose=True
            )

        # ì„¸ì…˜ ìƒíƒœì— QA ì²´ì¸ì´ ì—†ì„ ê²½ìš° ì´ˆê¸°í™”
        if "qa" not in st.session_state:
            qa = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type='stuff',  # or 'map_reduce', 'refine' if ë¬¸ì„œê°€ ê¸¸ë‹¤ë©´
                retriever=retriever,
            )
            st.session_state.qa = qa
        # PDF ì²˜ë¦¬ ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
        st.session_state['text_processed'] = True

# chat interface
response_container = st.container()

# í…ìŠ¤íŠ¸ ì²˜ë¦¬ëëŠ”ì§€ ì²´í¬
is_text_processed = st.session_state.get('text_processed', False)

with st.form(key='chat_form', clear_on_submit=True):
    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.text_input("Ask a question about the PDF content:", 
                               key="user_input",
                               disabled=not is_text_processed,)
    submit_button = st.form_submit_button(label='Send', 
                                          disabled=not is_text_processed)
    if submit_button and user_input:
        # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state['history'].append({"role": "user", "content": user_input})
        
        # LLMì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°
        with st.spinner("Generating response..."):
            response = st.session_state.qa.invoke({"query": user_input})['result']
        
        # ë‹µë³€ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state['history'].append({"role": "assistant", "content": response})
        

with response_container:
    for i, (user_msg, bot_msg) in enumerate(zip(st.session_state['history'][::2], st.session_state['history'][1::2])):
        message(user_msg['content'], is_user=True, key=f"user_{i}")
        message(bot_msg['content'], key=f"bot_{i}")
import os
import streamlit as st
from streamlit_chat import message
from rag_engine import process_pdf

st.title("PDF Chatbot for Data Drift")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'history' not in st.session_state:
    st.session_state['history'] = []

uploaded_file = st.file_uploader(" ", type=["pdf", "txt", "docx"])

model_options = ["ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", 
                 "yi:34b-chat", "llama3", "mistral", "phi3", "exaone3.5:7.8b",
                 "bnksys/yanolja-eeve-korean-instruct-10.8b:latest",
                 "jinbora/deepseek-r1-Bllossom:8b", "granite3.3:8b", "ggml", "kollama"]
selected_model = st.radio(
    "â© ë‹µë³€ì— ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.",
    model_options,
    key="model_select",
    horizontal=True
)

# ì‹¤ì œ ëª¨ë¸ ì„ íƒ ì‹œì—ë§Œ ë™ì‘í•˜ë„ë¡ ë¶„ê¸°
if uploaded_file is not None and selected_model != "ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”":
    max_size_mb = 1024
    if uploaded_file.size > max_size_mb * 1024 * 1024:
        st.error(f"File size exceeds {max_size_mb}MB limit.")
        st.stop()

    st.success("ğŸ˜€ File uploaded successfully!")
    safe_filename = os.path.basename(uploaded_file.name).replace(" ", "_")
    temp_file_path = os.path.join("temp", safe_filename)
    os.makedirs("temp", exist_ok=True)

    with open(temp_file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.info("ğŸ’¾ File saved to temp directory.")

    # ëª¨ë¸ ë˜ëŠ” íŒŒì¼ì´ ë°”ë€Œë©´ QA ì²´ì¸ ìƒˆë¡œ ìƒì„±
    if (
        "last_uploaded_file" not in st.session_state or
        st.session_state["last_uploaded_file"] != safe_filename or
        st.session_state.get("selected_model") != selected_model
    ):
        with st.spinner("ğŸ“š Loading PDF and initializing QA..."):
            qa_chain, n_chunks = process_pdf(temp_file_path, model_name=selected_model)
            st.session_state.qa = qa_chain
            st.session_state["last_uploaded_file"] = safe_filename
            st.session_state["selected_model"] = selected_model
            st.session_state['text_processed'] = True

            # ìµœì´ˆ 1íšŒ ì¸ì‚¬ë§ ì„¤ì •
            if 'initialized' not in st.session_state:
                st.session_state['history'] = [
                    {"role": "user", "content": "PDF Ready!"},
                    {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! PDFì—ì„œ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš” ğŸ˜Š"}
                ]
                st.session_state['initialized'] = True
                st.rerun()

# ì‘ë‹µ ì¶œë ¥
with st.container():
    for i, (user_msg, bot_msg) in enumerate(zip(st.session_state['history'][::2], st.session_state['history'][1::2])):
        message(user_msg['content'], is_user=True, key=f"user_{i}")
        message(bot_msg['content'], key=f"bot_{i}")

# ì‚¬ìš©ì ì…ë ¥ì°½
is_text_processed = st.session_state.get('text_processed', False)
user_input = st.text_input("Ask a question about the PDF content:", key="user_input", disabled=not is_text_processed)

if st.button("Send", disabled=not is_text_processed) and user_input:
    st.session_state['history'].append({"role": "user", "content": user_input})

    chat_history = [
        (st.session_state['history'][i]["content"], st.session_state['history'][i+1]["content"])
        for i in range(0, len(st.session_state['history']) - 1, 2)
    ]

    with st.spinner("ğŸ¤” Generating response..."):
        result = st.session_state.qa.invoke({
            "question": user_input,
            "chat_history": chat_history
        })
        response = result['answer']

    st.session_state['history'].append({"role": "assistant", "content": response})

    st.rerun()

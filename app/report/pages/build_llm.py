import streamlit as st
import subprocess
import json
from langchain_community.llms import Ollama

# --------------------- Ollama base LLM ì„¤ì • - llm ---------------------
def get_model():
    if not st.session_state.get('model_configured'):
        return None
    
    try:
        llm = Ollama(
            model=st.session_state.get('model_name'),
            temperature=st.session_state.get('model_temperature', 0.7)
        )
        return llm
    except Exception as e:
        st.error(f"LLM ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# --------------------- Ollama default prompt ---------------------
def get_default_drift_prompt():
    return """ë‹¹ì‹ ì€ ë°ì´í„° ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë“œë¦¬í”„íŠ¸ ë¶„ì„ í•´ì„¤ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

            ë°ì´í„°ì…‹: {dataset_name}
            ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼: {drift_summary}
            ì„ë² ë”© ì •ë³´: {embedding_info}

            ì°¸ê³  ì§€ì‹:
            {context}

            ë‹¤ìŒ 4ë‹¨ê³„ êµ¬ì¡°ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

            [ê¸°ìˆ ì  ë¶„ì„] ê° ë“œë¦¬í”„íŠ¸ ë©”íŠ¸ë¦­ì˜ ìˆ˜ì¹˜ì  ì˜ë¯¸ë¥¼ í•´ì„í•©ë‹ˆë‹¤.
            [í˜„ ìƒí™© ë¶„ì„] í˜„ì¬ ë“œë¦¬í”„íŠ¸ ìƒí™©ì´ ëª¨ë¸ì— ë¯¸ì¹  ì˜í–¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
            [ì‹œê°ì  ë¶„ì„] PCA ì‹œê°í™” ê²°ê³¼ì™€ ì—°ê³„í•˜ì—¬ ë°ì´í„° ë¶„í¬ ë³€í™”ë¥¼ í•´ì„í•©ë‹ˆë‹¤.
            [ê¶Œì¥ì‚¬í•­] ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ê³¼ ëª¨ë‹ˆí„°ë§ ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.

            ê° ë‹¨ê³„ëŠ” 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""

# --------------------- Ollama LLM answer example ---------------------
def generate_drift_explanation_preview():
    try:
        # ì„¸ì…˜ì—ì„œ ë“œë¦¬í”„íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        dataset_name = st.session_state.get('selected_dataset', 'Test Dataset')
        drift_summary = st.session_state.get('drift_summary', 'ë“œë¦¬í”„íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
        
        if drift_summary == 'ë“œë¦¬í”„íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.':
            return None
        
        # Custom LLM ì‚¬ìš©
        llm = get_model()
        if not llm:
            return None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        prompt_template = st.session_state.get('custom_prompt_template', 
                                               get_default_drift_prompt())
        
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        formatted_prompt = prompt_template.format(
            dataset_name=dataset_name,
            drift_summary=drift_summary,
            embedding_info="Preview mode - ì‹¤ì œ ì„ë² ë”© ì •ë³´",
            context="Preview mode - RAG ê²€ìƒ‰ ê²°ê³¼"
        )
        
        # LLM í˜¸ì¶œ
        explanation = llm.invoke(formatted_prompt)
        
        return f"""
        <div style="background-color: #e8f5e8; border-left: 4px solid #28a745; padding: 20px; margin: 10px 0;">
            <h4 style="margin-top: 0; color: #28a745;">ğŸ¤– AI ë“œë¦¬í”„íŠ¸ ë¶„ì„ í•´ì„¤ (ë¯¸ë¦¬ë³´ê¸°)</h4>
            <div style="line-height: 1.8; font-size: 14px;">
                {explanation.replace(chr(10), '<br>')}
            </div>
            <small style="color: #6c757d; font-style: italic; margin-top: 15px; display: block;">
                * ì´ëŠ” ë¯¸ë¦¬ë³´ê¸°ì…ë‹ˆë‹¤. ì‹¤ì œ Generate Reportì—ì„œ ì „ì²´ ë¶„ì„ì´ ì§„í–‰ë©ë‹ˆë‹¤.
            </small>
        </div>
        """
        
    except Exception as e:
        st.error(f"ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# --------------------- Ollama ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ---------------------
def get_ollama_models():
    try:
        # ollama list ëª…ë ¹ì–´ ì‹¤í–‰
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True,
            check=True
        )
        
        models = []
        lines = result.stdout.strip().split('\n')
        
        # í—¤ë” ì œê±°í•˜ê³  ëª¨ë¸ëª…ë§Œ ì¶”ì¶œ
        for line in lines[1:]:  # ì²« ë²ˆì§¸ ì¤„ì€ í—¤ë”
            if line.strip():
                model_name = line.split()[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ëª¨ë¸ëª…
                if model_name and not model_name.startswith('NAME'):
                    models.append(model_name)
        
        return models
        
    except subprocess.CalledProcessError:
        st.warning("Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šê±°ë‚˜ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    except FileNotFoundError:
        st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    except Exception as e:
        st.warning(f"ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return []

# --------------------- main : Custom LLM ì„¤ì • í˜ì´ì§€ ---------------------
def render():
    # í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹ í™•ì¸
    st.session_state['dataset_name'] = st.session_state.get('selected_dataset')
    dataset_name = st.session_state['dataset_name'] 

    if not dataset_name:
        st.warning("âš ï¸ ë¨¼ì € ê²°ê³¼ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ============ 1. ëª¨ë¸ ì„¤ì • ì„¹ì…˜ ============
    col1, col2, col3, col4 = st.columns(4)
        
    with col1:
        # Ollama ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_models = get_ollama_models()
        
        if not available_models:
            st.error("âŒ Ollama ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.code("ollama list", language="bash")
            return
        
        selected_model = st.selectbox(
                "ğŸ¯ Model Name",
                ["Select LLM"] + available_models,
                key="model_selection",
                help="ë“œë¦¬í”„íŠ¸ ë¶„ì„ í•´ì„¤ì„ ìƒì„±í•  LLM ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤."
            )
        
    with col2:
        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            key="temperature_slider",
            help="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€"
        )
            
    with col3:
        top_p = st.slider(
            "Top P",
            min_value=0.1,
            max_value=1.0,
            value=0.9,
            step=0.1,
            help="ëˆ„ì  í™•ë¥  ì„ê³„ê°’"
            )

    with col4:
        max_tokens = st.number_input(
            "Max Tokens",
            min_value=100,
            max_value=4000,
            value=1000,
            step=100,
            help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜"
            )

    # ============ 2. í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì„¹ì…˜ ============
    custom_prompt = st.text_area(
            "Prompt Template",
            value=get_default_drift_prompt(),
            height=450,
            help="í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”."
        )
    with st.expander("ğŸ“– Input Variables"):
        st.code("""
            {dataset_name} - ì„ íƒëœ ë°ì´í„°ì…‹ ì´ë¦„
            {drift_summary} - ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½
            {embedding_info} - ì„ë² ë”© ì •ë³´
            {context} - RAGì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì§€ì‹ """)
    
    # LLM ì„¤ì • ì €ì¥
    if st.button("âœ… LLM ì„¤ì • ì™„ë£Œ", key="save_llm_config"):
        # ëª¨ë¸ ì„ íƒ ì—¬ë¶€ í™•ì¸
        if selected_model == "Select LLM":
            st.warning("âš ï¸ ëª¨ë¸ì„ ì¬ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
            
        try:
            # LLM ì„¤ì • ë³€ìˆ˜ë“¤ - Ollama ëª¨ë¸ë§Œ ì§€ì›
            st.session_state['llm_generation_params'] = {
                'selected_model': selected_model,
                'temperature': temperature,
                'custom_prompt': custom_prompt
            }

            st.session_state.model_name = selected_model
            st.session_state.model_temperature = temperature
            st.session_state.model_max_tokens = max_tokens
            st.session_state.model_top_p = top_p
            st.session_state.model_configured = True
            st.session_state.custom_prompt_template = custom_prompt

            # LLM ê°ì²´ ìƒì„± ë° í…ŒìŠ¤íŠ¸
            llm = Ollama(
                model=selected_model,
                temperature=temperature
            )
            
            st.success("ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! Generate Reportì—ì„œ ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”!")

        except Exception as e:
            st.error(f"âŒ ëª¨ë¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

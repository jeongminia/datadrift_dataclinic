import streamlit as st
import subprocess
import json
from langchain_ollama import OllamaLLM

def get_default_drift_prompt():
    """ê¸°ë³¸ ë“œë¦¬í”„íŠ¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸"""
    return """ë‹¹ì‹ ì€ ë°ì´í„° ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë“œë¦¬í”„íŠ¸ ë¶„ì„ í•´ì„¤ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

            ë°ì´í„°ì…‹: {dataset_name}
            ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼: {drift_summary}
            ì„ë² ë”© ì •ë³´: {embedding_info}

            ì°¸ê³  ì§€ì‹:
            {context}

            ë‹¤ìŒ 4ë‹¨ê³„ êµ¬ì¡°ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

            [ê¸°ìˆ ì  ë¶„ì„] ê° ë“œë¦¬í”„íŠ¸ ë©”íŠ¸ë¦­ì˜ ìˆ˜ì¹˜ì  ì˜ë¯¸ë¥¼ í•´ì„í•©ë‹ˆë‹¤.
            [í˜„ ìƒí™© ë¶„ì„] í˜„ì¬ ë“œë¦¬í”„íŠ¸ ìƒí™©ì´ ëª¨ë¸ì— ë¯¸ì¹  ì˜í–¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
            [ì‹œê°ì  ë¶„ì„] PCA ì‹œê°í™” ê²°ê³¼ì™€ ì—°ê³„í•˜ì—¬ ë°ì´í„° ë¶„í¬ ë³€í™”ë¥¼ í•´ì„í•©ë‹ˆë‹¤.
            [ê¶Œì¥ì‚¬í•­] ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ê³¼ ëª¨ë‹ˆí„°ë§ ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.

            ê° ë‹¨ê³„ëŠ” 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""

def generate_drift_explanation_preview():
    """ë“œë¦¬í”„íŠ¸ í•´ì„¤ ë¯¸ë¦¬ë³´ê¸° ìƒì„±"""
    try:
        # ì„¸ì…˜ì—ì„œ ë“œë¦¬í”„íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        dataset_name = st.session_state.get('selected_dataset', 'Test Dataset')
        drift_summary = st.session_state.get('drift_summary', 'ë“œë¦¬í”„íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
        
        if drift_summary == 'ë“œë¦¬í”„íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.':
            return None
        
        # Custom LLM ì‚¬ìš©
        llm = get_custom_llm()
        if not llm:
            return None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        prompt_template = st.session_state.get('custom_prompt_template', get_default_drift_prompt())
        
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        formatted_prompt = prompt_template.format(
            dataset_name=dataset_name,
            drift_summary=drift_summary,
            embedding_info="Preview mode - ì‹¤ì œ ì„ë² ë”© ì •ë³´",
            context="Preview mode - RAG ê²€ìƒ‰ ê²°ê³¼"
        )
        
        # LLM í˜¸ì¶œ
        explanation = llm.invoke(formatted_prompt)
        
        return f"""
        <div style="background-color: #e8f5e8; border-left: 4px solid #28a745; padding: 20px; margin: 10px 0;">
            <h4 style="margin-top: 0; color: #28a745;">ğŸ¤– AI ë“œë¦¬í”„íŠ¸ ë¶„ì„ í•´ì„¤ (ë¯¸ë¦¬ë³´ê¸°)</h4>
            <div style="line-height: 1.8; font-size: 14px;">
                {explanation.replace(chr(10), '<br>')}
            </div>
            <small style="color: #6c757d; font-style: italic; margin-top: 15px; display: block;">
                * ì´ëŠ” ë¯¸ë¦¬ë³´ê¸°ì…ë‹ˆë‹¤. ì‹¤ì œ Generate Reportì—ì„œ ì „ì²´ ë¶„ì„ì´ ì§„í–‰ë©ë‹ˆë‹¤.
            </small>
        </div>
        """
        
    except Exception as e:
        st.error(f"ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def get_ollama_models():
    """ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # ollama list ëª…ë ¹ì–´ ì‹¤í–‰
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True,
            check=True
        )
        
        models = []
        lines = result.stdout.strip().split('\n')
        
        # í—¤ë” ì œê±°í•˜ê³  ëª¨ë¸ëª…ë§Œ ì¶”ì¶œ
        for line in lines[1:]:  # ì²« ë²ˆì§¸ ì¤„ì€ í—¤ë”
            if line.strip():
                model_name = line.split()[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ëª¨ë¸ëª…
                if model_name and not model_name.startswith('NAME'):
                    models.append(model_name)
        
        return models
        
    except subprocess.CalledProcessError:
        st.warning("Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šê±°ë‚˜ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    except FileNotFoundError:
        st.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    except Exception as e:
        st.warning(f"ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def get_custom_llm():
    """ì„¤ì •ëœ Custom LLM ê°ì²´ ë°˜í™˜"""
    if not st.session_state.get('custom_llm_configured'):
        return None
    
    try:
        llm = OllamaLLM(
            model=st.session_state.get('custom_llm_model'),
            temperature=st.session_state.get('custom_llm_temperature', 0.7)
        )
        return llm
    except Exception as e:
        st.error(f"LLM ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def render():
    """Custom LLM ì„¤ì • í˜ì´ì§€"""
    st.markdown("""
            <div style="background: linear-gradient(135deg, #c7a2ff 0%, #9b7bff 50%, #b797ff 100%);
                        padding: 15px 25px; border-radius: 15px; margin-bottom: 25px;
                        box-shadow: 0 6px 20px rgba(255, 107, 107, 0.4);
                        border: 1px solid rgba(255, 255, 255, 0.3);
                        transform: scale(0.95);">
                <div style="text-align: center;">
                    <h3 style="color: white; margin: 0; font-weight: 700; font-size: 18px; 
                            text-shadow: 0 2px 4px rgba(0,0,0,0.3);">
                        Custom LLM Configuration
                    </h3>
                    <div style="color: rgba(255, 255, 255, 0.95); font-size: 12px; margin-top: 8px;
                            text-shadow: 0 1px 2px rgba(0,0,0,0.2);">
                        ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ë§ì¶¤í˜• í•´ì„¤ì„ ìœ„í•´ AI ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
    
    # í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹ í™•ì¸
    selected_dataset = st.session_state.get('selected_dataset')
    if not selected_dataset:
        st.warning("âš ï¸ ë¨¼ì € ê²°ê³¼ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ============ 1. ëª¨ë¸ ì„¤ì • ì„¹ì…˜ ============
    st.markdown("#### 1. Select AI Model & Settings")
    
    # Ollama ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    available_models = get_ollama_models()
    
    if not available_models:
        st.error("âŒ Ollama ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.code("ollama list", language="bash")
        return
    
    # ëª¨ë¸ ì„ íƒ UI
    col1, col2 = st.columns([2, 1])
    
    with col1:
        selected_model = st.selectbox(
            "ğŸ¯ ì‚¬ìš©í•  AI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
            available_models,
            key="model_selection",
            help="ë“œë¦¬í”„íŠ¸ ë¶„ì„ í•´ì„¤ì„ ìƒì„±í•  AI ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤."
        )
    
    with col2:
        temperature = st.slider(
            "ğŸŒ¡ï¸ Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            key="temperature_slider",
            help="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€"
        )

    col1, col2 = st.columns(2)
        
    with col1:
        max_tokens = st.number_input(
            "Max Tokens",
            min_value=100,
            max_value=4000,
            value=1000,
            step=100,
            help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜"
            )
            
    with col2:
        top_p = st.slider(
            "Top P",
            min_value=0.1,
            max_value=1.0,
            value=0.9,
            step=0.1,
            help="ëˆ„ì  í™•ë¥  ì„ê³„ê°’"
            )
    
    # LLM ì„¤ì • ì €ì¥
    if st.button("âœ… LLM ì„¤ì • ì™„ë£Œ", type="primary", key="save_llm_config"):
        try:
            # LLM ê°ì²´ ìƒì„± ë° í…ŒìŠ¤íŠ¸
            llm = OllamaLLM(
                model=selected_model,
                temperature=temperature
            )
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            test_response = llm.invoke("Hello")
            
            # ì„¸ì…˜ì— ì €ì¥
            st.session_state.custom_llm_model = selected_model
            st.session_state.custom_llm_temperature = temperature
            st.session_state.custom_llm_max_tokens = max_tokens
            st.session_state.custom_llm_top_p = top_p
            st.session_state.custom_llm_configured = True
        
        except Exception as e:
            st.error(f"âŒ ëª¨ë¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    # ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì—¬ê¸°ì„œ ì¢…ë£Œ
    if not st.session_state.get('custom_llm_configured'):
        return

    
    # ============ 2. í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì„¹ì…˜ ============
    st.markdown("#### 2. Customize Prompt Template")
    
    # í”„ë¦¬ì…‹ ì„ íƒ
    preset_options = {
        "ê¸°ë³¸ ë“œë¦¬í”„íŠ¸ ë¶„ì„": get_default_drift_prompt(),
        "ì‚¬ìš©ì ì •ì˜": ""
    }
    
    selected_preset = st.selectbox(
        "ğŸ“‹ í”„ë¡¬í”„íŠ¸ í”„ë¦¬ì…‹ ì„ íƒ:",
        list(preset_options.keys()),
        help="ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì„ íƒí•˜ê±°ë‚˜ ì§ì ‘ ì‘ì„±í•˜ì„¸ìš”."
    )
    
    # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì„¤ëª…
    with st.expander("ğŸ“– ì‚¬ìš© ê°€ëŠ¥í•œ ë³€ìˆ˜"):
        st.code("""
            {dataset_name} - ì„ íƒëœ ë°ì´í„°ì…‹ ì´ë¦„
            {drift_summary} - ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½
            {embedding_info} - ì„ë² ë”© ì •ë³´
            {context} - RAGì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì§€ì‹
                    """)

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í¸ì§‘
    if selected_preset == "ì‚¬ìš©ì ì •ì˜":
        custom_prompt = st.text_area(
            "ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸:",
            height=250,
            placeholder="ì—¬ê¸°ì— ì›í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‘ì„±í•˜ì„¸ìš”...",
            help="ë³€ìˆ˜: {dataset_name}, {drift_summary}, {embedding_info}"
        )
    else:
        custom_prompt = st.text_area(
            f"{selected_preset} í”„ë¡¬í”„íŠ¸:",
            value=preset_options[selected_preset],
            height=250,
            help="í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”."
        )
    
    # í”„ë¡¬í”„íŠ¸ ì €ì¥
    if st.button("ğŸ’¾ í”„ë¡¬í”„íŠ¸ ì €ì¥", key="save_prompt"):
        st.session_state.custom_prompt_template = custom_prompt
    
    # í”„ë¡¬í”„íŠ¸ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì œí•œì ìœ¼ë¡œ í‘œì‹œ
    if not st.session_state.get('custom_prompt_template'):
        return
    
    # ============ 3. ì„¤ì • í™•ì¸ ë° ë¯¸ë¦¬ë³´ê¸° ì„¹ì…˜ ============
    st.markdown("#### 3. Check Settings")
    
    # í˜„ì¬ ì„¤ì • í‘œì‹œ
    config_col1, config_col2, config_col3 = st.columns(3)
    
    with config_col1:
        st.metric("Model Name", st.session_state.get('custom_llm_model', 'N/A'))
        
    with config_col2:
        st.metric("Temperature", st.session_state.get('custom_llm_temperature', 'N/A'))
        
    with config_col3:
        st.metric("State", "âœ…")
    
    # ëª¨ë“  ì„¤ì • ì™„ë£Œ ìƒíƒœ í™•ì¸
    all_configured = (
        st.session_state.get('custom_llm_configured') and 
        st.session_state.get('custom_prompt_template')
    )
    
    if all_configured:
        st.success("ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! Generate Reportì—ì„œ ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”!")
        
    else:
        st.info("âš ï¸ ëª¨ë“  ì„¤ì •ì„ ì™„ë£Œí•˜ë©´ Generate Reportë¡œ ì´ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
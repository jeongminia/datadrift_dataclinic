import os
import contextlib
import tempfile
from llama_cpp import Llama

@contextlib.contextmanager
def suppress_stdout_stderr():
    """C-level stdout/stderr ì–µì œìš© context manager"""
    with tempfile.TemporaryFile() as fnull:
        fd_stdout = os.dup(1)
        fd_stderr = os.dup(2)
        os.dup2(fnull.fileno(), 1)
        os.dup2(fnull.fileno(), 2)
        try:
            yield
        finally:
            os.dup2(fd_stdout, 1)
            os.dup2(fd_stderr, 2)

# ëª¨ë¸ ê²½ë¡œ
model_path = "/home/keti/datadrift_jm/models/gpt4all/ggml-model-Q4_K_M.gguf"

# suppress + verbose=False
with suppress_stdout_stderr():
    model = Llama(
        model_path=model_path,
        n_ctx=2048,
        n_threads=8,
        verbose=False
    )

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
test_prompt = """
ì´ ë¬¸ì„œ ìˆ˜: 4078
í‰ê·  ë¬¸ì¥ ê¸¸ì´: 13 ë‹¨ì–´
ì£¼ìš” í‚¤ì›Œë“œ: ì‚´ì¸, í”¼ê³ ì¸, í”¼í•´ì, ì¦ê±°, íŒê²°
"""

full_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë“œë¦¬í”„íŠ¸ë¥¼ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‚¬ìš©ìë¥¼ ë•ëŠ” ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.
ë°ì´í„° ë“œë¦¬í”„íŠ¸ë€ ì…ë ¥ ë°ì´í„°ì˜ ë³€í™”ë¡œ ì¸í•´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.

train, test, validation ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë°ì´í„° ë“œë¦¬í”„íŠ¸ ë¶„ì„ì„ ìœ„í•´ ì‹œê°í™” ë° EDA ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì„¸ìš”.
ë‹¤ìŒ ë°ì´í„° í†µê³„ë¥¼ ë³´ê³  ë¶„ì„ ê²°ê³¼ë¥¼ 5ë¬¸ì¥ ì´ë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

{test_prompt}

â†’ ë¶„ì„ ìš”ì•½:
"""

# ì‘ë‹µ ìƒì„±
response = model(
    full_prompt,
    max_tokens=300,
    temperature=0.7,
    top_p=0.9,
    repeat_penalty=1.1
)

# ì›í•˜ëŠ” ì¶œë ¥ë§Œ!
print("ğŸ“Œ í…ŒìŠ¤íŠ¸ ì‘ë‹µ:", response["choices"][0]["text"].strip())

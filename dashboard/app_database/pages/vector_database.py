import os
import time
import json
import numpy as np
import pandas as pd
import streamlit as st
from pymilvus import utility, Collection, CollectionSchema, FieldSchema, DataType, connections
from ..utils import EmbeddingPipeline, split_columns, get_data_from_session

# Milvus ì„œë²„ì— ì—°ê²°
connections.connect("default", host="localhost", port="19530")

# ----------------------------- Vector Database Functions -----------------------------
## MilvusDB Schema
def create_collection(collection_name):
    """í†µí•© ì»¬ë ‰ì…˜ ìƒì„± (ë°ì´í„° + ë©”íƒ€ë°ì´í„°)"""
    if not utility.has_collection(collection_name):
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="set_type", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="class", dtype=DataType.VARCHAR, max_length=50), 
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=768),
            # ë©”íƒ€ë°ì´í„° í•„ë“œë“¤
            FieldSchema(name="dataset_name", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="summary_dict", dtype=DataType.VARCHAR, max_length=20000),
            FieldSchema(name="data_previews", dtype=DataType.VARCHAR, max_length=20000),
            FieldSchema(name="class_dist_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="doc_len_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="doc_len_table", dtype=DataType.VARCHAR, max_length=10000),
            FieldSchema(name="wordcloud_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="timestamp", dtype=DataType.INT64),
            # ë°ì´í„° ë“œë¦¬í”„íŠ¸ í•„ë“œ
            FieldSchema(name="dimension", dtype=DataType.FLOAT),
            FieldSchema(name="embedding_size", dtype=DataType.VARCHAR, max_length=5000),
            FieldSchema(name="original_distance_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="PCA_distance_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="PCA_visualization_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="drift_score_summary", dtype=DataType.VARCHAR, max_length=10000),

        ]
        schema = CollectionSchema(fields=fields, description="Text Embeddings with Metadata")
        collection = Collection(name=collection_name, schema=schema)
        collection.create_index(
            field_name="vector", 
            index_params={"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": 128}}
        )
        st.write(f"âœ… Created collection: {collection_name}")
    else:
        collection = Collection(name=collection_name)
    collection.load()
    return collection

# ----------------------------------- Prepare Metadata ----------------------------------- 
## ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜: ë³µì¡í•œ ê°ì²´ë“¤ì„ ë¬¸ìì—´ì´ë‚˜ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
def make_json_serializable(obj):
    try:
        if obj is None:
            return None
        elif isinstance(obj, (str, int, float, bool)):
            return obj
        elif isinstance(obj, dict):
            return {k: make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [make_json_serializable(item) for item in obj]
        elif isinstance(obj, pd.DataFrame):
            df_copy = obj.copy()
            for col in df_copy.columns:
                # ì»¬ëŸ¼ì´ ë²”ì£¼í˜•ì´ë©´ ë¬¸ìì—´ë¡œ ë³€í™˜
                if pd.api.types.is_categorical_dtype(df_copy[col]):
                    df_copy[col] = df_copy[col].astype(str)
                # NaN ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
                df_copy[col] = df_copy[col].fillna("")
            return df_copy.to_dict("records")
        elif isinstance(obj, pd.Series):
            # Categorical Seriesë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if pd.api.types.is_categorical_dtype(obj):
                return obj.astype(str).fillna("").tolist()
            return obj.fillna("").tolist()
        elif isinstance(obj, pd.Categorical):
            # Categorical ê°ì²´ë¥¼ ì§ì ‘ ì²˜ë¦¬
            return obj.astype(str).tolist()
        elif isinstance(obj, pd.CategoricalDtype):
            # CategoricalDtype ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            return str(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.generic):
            return obj.item()
        elif pd.isna(obj):
            return ""
        elif hasattr(obj, 'dtype'):
            # pandas dtypeì´ ìˆëŠ” ê°ì²´ë“¤ ì²˜ë¦¬
            if 'category' in str(obj.dtype):
                return str(obj)
            elif 'object' in str(obj.dtype):
                return str(obj)
        elif hasattr(obj, 'to_dict'):
            return obj.to_dict()
        elif hasattr(obj, 'tolist'):
            return obj.tolist()
        else:
            return str(obj)
    except Exception as e:
        # ëª¨ë“  ì˜ˆì™¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        return str(obj)
        
## Session Stateì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í›„ ì§ë ¬í™”
def prepare_metadata():
    summary_dict = st.session_state.get("dataset_summary", {})
    class_dist_path = st.session_state.get("class_dist_path", "")
    doc_len_path = st.session_state.get("doc_len_path", "")
    doc_len_table = st.session_state.get("doc_len_table", "")
    wordcloud_path = st.session_state.get("wordcloud_path", "")
    dataset_name = st.session_state.get("dataset_name", "")
        
    train_df, valid_df, test_df = get_data_from_session()
    data_previews = {}
        
    if train_df is not None and len(train_df) > 0:
        # Categorical ì»¬ëŸ¼ì„ ë¯¸ë¦¬ ì²˜ë¦¬í•œ í›„ preview ìƒì„±
        train_preview = train_df.head(5).copy()
        for col in train_preview.columns:
            if pd.api.types.is_categorical_dtype(train_preview[col]):
                train_preview[col] = train_preview[col].astype(str)
        preview_data = make_json_serializable(train_preview.to_dict('records'))
        data_previews["train"] = {"data": preview_data,
                                "total_rows": len(train_df),
                                "columns": list(train_df.columns)
                                }
        
    if valid_df is not None and len(valid_df) > 0:
        # Categorical ì»¬ëŸ¼ì„ ë¯¸ë¦¬ ì²˜ë¦¬í•œ í›„ preview ìƒì„±
        valid_preview = valid_df.head(5).copy()
        for col in valid_preview.columns:
            if pd.api.types.is_categorical_dtype(valid_preview[col]):
                valid_preview[col] = valid_preview[col].astype(str)
        preview_data = make_json_serializable(valid_preview.to_dict('records'))
        data_previews["valid"] = {"data": preview_data,
                                "total_rows": len(valid_df),
                                "columns": list(valid_df.columns)
                                }
            
    if test_df is not None and len(test_df) > 0:
        # Categorical ì»¬ëŸ¼ì„ ë¯¸ë¦¬ ì²˜ë¦¬í•œ í›„ preview ìƒì„±
        test_preview = test_df.head(5).copy()
        for col in test_preview.columns:
            if pd.api.types.is_categorical_dtype(test_preview[col]):
                test_preview[col] = test_preview[col].astype(str)
        preview_data = make_json_serializable(test_preview.to_dict('records'))
        data_previews["test"] = {"data": preview_data,
                                "total_rows": len(test_df),
                                "columns": list(test_df.columns)
                                }

    paths = {"class_dist_path": os.path.abspath(class_dist_path) if class_dist_path else "",
            "doc_len_path": os.path.abspath(doc_len_path) if doc_len_path else "",
            "wordcloud_path": os.path.abspath(wordcloud_path) if wordcloud_path else ""
            }
        
    # ì•ˆì „í•œ JSON ì§ë ¬í™”
    summary_dict_json = ""
    doc_len_table_json = ""
    data_previews_json = ""
        
    if summary_dict:
        try:
            summary_dict_json = json.dumps(make_json_serializable(summary_dict), default=str)
        except Exception as e:
            st.warning(f"summary_dict ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            summary_dict_json = json.dumps(str(summary_dict))
        
    if doc_len_table:
        try:
            doc_len_table_json = json.dumps(make_json_serializable(doc_len_table), default=str)
        except Exception as e:
            st.warning(f"doc_len_table ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            doc_len_table_json = json.dumps(str(doc_len_table))
        
    if data_previews:
        try:
            data_previews_json = json.dumps(make_json_serializable(data_previews), default=str)
        except Exception as e:
            st.warning(f"data_previews ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            data_previews_json = json.dumps(str(data_previews))
        
    return {
            "dataset_name": dataset_name,
            "summary_dict": summary_dict_json,
            "data_previews": data_previews_json,
            "class_dist_path": paths["class_dist_path"],
            "doc_len_path": paths["doc_len_path"],
            "doc_len_table": doc_len_table_json,
            "wordcloud_path": paths["wordcloud_path"],
            "timestamp": int(time.time())
        }

# ----------------------------------- Save Metadata ----------------------------------- 
def save_metadata_to_vectordb(collection_name):
    collection = Collection(name=collection_name)
    collection.load()
        
    # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì‚­ì œ
    metadata_results = collection.query(
            expr="set_type == 'metadata'",
            output_fields=["id"],
            limit=1
        )
        
    if metadata_results:
        metadata_id = metadata_results[0]["id"]
        collection.delete(f"id == {metadata_id}")
        
    # ìƒˆ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
    metadata = prepare_metadata()
    dummy_vector = [0.0] * 768
        
    # ë©”íƒ€ë°ì´í„° ì‚½ì… (ë°ì´í„° ë“œë¦¬í”„íŠ¸ í•„ë“œ í¬í•¨)
    data = [
            ["metadata"],
            ["metadata"],
            [dummy_vector],
            [metadata["dataset_name"]],
            [metadata["summary_dict"]],
            [metadata["data_previews"]],
            [metadata["class_dist_path"]],
            [metadata["doc_len_path"]],
            [metadata["doc_len_table"]],
            [metadata["wordcloud_path"]],
            [metadata["timestamp"]],
            # ë°ì´í„° ë“œë¦¬í”„íŠ¸ í•„ë“œë“¤ì„ ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            [0.0],  # dimension
            [""],   # embedding_size
            [""],   # original_distance_path
            [""],   # PCA_distance_path
            [""],   # PCA_visualization_path
            [""],   # drift_score_summary
        ]
        
    fields = ["set_type", "class", "vector", "dataset_name", "summary_dict", 
                 "data_previews", "class_dist_path", "doc_len_path", "doc_len_table", "wordcloud_path", "timestamp",
                 "dimension", "embedding_size", "original_distance_path", 
                 "PCA_distance_path", "PCA_visualization_path", "drift_score_summary"]
        
    result = collection.insert(data, fields=fields)
    collection.flush()
        
    st.success(f"âœ… ë©”íƒ€ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return result.primary_keys[0]

# ----------------------------------- Save Vector ----------------------------------- 
## text Embedding ë²¡í„°ë¥¼ batch ë‹¨ìœ„ë¡œ ë²¡í„°DBì— ì €ì¥
def insert_vectors(collection_name, vectors, set_type, class_labels, batch_size=500, metadata=None):
    collection = Collection(name=collection_name)
    class_labels = [str(label) if not isinstance(label, str) else label for label in class_labels]
    total = len(vectors)
    ids = []
    
    # ë©”íƒ€ë°ì´í„° ì‚½ì… (train ë°ì´í„°ì™€ í•¨ê»˜ ì²˜ë¦¬)
    if metadata and set_type == "train":
        dummy_vector = [0.0] * 768
        
        metadata_data = [
            ["metadata"],
            ["metadata"],
            [dummy_vector],
            [metadata["dataset_name"]],
            [metadata["summary_dict"]],
            [metadata["data_previews"]], 
            [metadata["class_dist_path"]],
            [metadata["doc_len_path"]],
            [metadata["doc_len_table"]],
            [metadata["wordcloud_path"]],
            [metadata["timestamp"]],
            # Drift ê´€ë ¨ í•„ë“œë“¤ì„ ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            [0.0],  # dimension
            [""],   # embedding_size
            [""],   # original_distance_path
            [""],   # PCA_distance_path
            [""],   # PCA_visualization_path
            [""],   # drift_score_summary
        ]
        
        fields = ["set_type", "class", "vector", "dataset_name", "summary_dict", 
                 "data_previews", "class_dist_path", "doc_len_path", "doc_len_table", "wordcloud_path", "timestamp",
                 "dimension", "embedding_size", "original_distance_path", 
                 "PCA_distance_path", "PCA_visualization_path", "drift_score_summary"]
        
        metadata_ids = collection.insert(metadata_data, fields=fields)
        ids.append(metadata_ids)
        collection.flush()
    
    # ì¼ë°˜ ë°ì´í„° ì‚½ì…
    for i in range(0, total, batch_size):
        batch_vectors = vectors[i:i+batch_size]
        batch_labels = class_labels[i:i+batch_size]
        batch_set_type = [set_type] * len(batch_vectors)
        
        # ë©”íƒ€ë°ì´í„° í•„ë“œë“¤ì€ ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •
        empty_metadata = [""] * len(batch_vectors)
        zero_timestamp = [0] * len(batch_vectors)
        zero_dimension = [0.0] * len(batch_vectors)  # Drift í•„ë“œìš©
        
        data = [
            batch_set_type,
            batch_labels,
            batch_vectors,
            empty_metadata,  # dataset_name
            empty_metadata,  # summary_dict
            empty_metadata,  # data_previews
            empty_metadata,  # class_dist_path
            empty_metadata,  # doc_len_path
            empty_metadata,  # doc_len_table
            empty_metadata,  # wordcloud_path
            zero_timestamp,  # timestamp
            zero_dimension,  # dimension
            empty_metadata,  # embedding_size
            empty_metadata,  # original_distance_path
            empty_metadata,  # PCA_distance_path
            empty_metadata,  # PCA_visualization_path
            empty_metadata,  # drift_score_summary
        ]
        
        fields = ["set_type", "class", "vector", "dataset_name", "summary_dict", 
                 "data_previews", "class_dist_path", "doc_len_path", "doc_len_table", "wordcloud_path", "timestamp",
                 "dimension", "embedding_size", "original_distance_path", 
                 "PCA_distance_path", "PCA_visualization_path", "drift_score_summary"]
        
        batch_ids = collection.insert(data, fields=fields)
        ids.append(batch_ids)
        collection.flush()
    
    return ids

# ----------------------------------- Load Metadata ----------------------------------- 
## JSON
def load_metadata_from_vectordb(collection_name):
    try:
        if not utility.has_collection(collection_name):
            return None
        
        collection = Collection(name=collection_name)
        collection.load()
        
        # ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ë°ì´í„° ë“œë¦¬í”„íŠ¸ í•„ë“œ í¬í•¨)
        results = collection.query(
            expr="set_type == 'metadata'",
            output_fields=["dataset_name", "summary_dict", "data_previews", "class_dist_path", 
                          "doc_len_path", "doc_len_table", "wordcloud_path", "timestamp",
                          "dimension", "embedding_size", "original_distance_path", 
                          "PCA_distance_path", "PCA_visualization_path", "drift_score_summary"],
            limit=1
        )
        
        if not results:
            return None
            
        result = results[0]
        
        # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ë¡œë“œ
        st.session_state["dataset_name"] = result["dataset_name"]
        st.session_state["class_dist_path"] = result["class_dist_path"]
        st.session_state["doc_len_path"] = result["doc_len_path"]
        st.session_state["wordcloud_path"] = result["wordcloud_path"]
        
        # JSON íŒŒì‹±
        if result["summary_dict"]:
            try:
                st.session_state["dataset_summary"] = json.loads(result["summary_dict"])
            except:
                st.session_state["dataset_summary"] = {}
        
        # data_previews íŒŒì‹± (JSON í˜•íƒœë¡œ ì €ì¥ë¨)
        if result.get("data_previews"):
            try:
                st.session_state["data_previews"] = json.loads(result["data_previews"])
            except:
                st.session_state["data_previews"] = {}
        else:
            st.session_state["data_previews"] = {}
        
        if result["doc_len_table"]:
            try:
                st.session_state["doc_len_table"] = json.loads(result["doc_len_table"])
            except:
                st.session_state["doc_len_table"] = result["doc_len_table"]
        else:
            st.session_state["doc_len_table"] = ""
        
        return result
        
    except Exception as e:
        st.error(f"âŒ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

## numpy 
def load_and_save_data(data, collection_name, set_type, class_labels, metadata=None):
    """ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë²¡í„°DBì— ì €ì¥"""
    create_collection(collection_name)
    vectors = np.array(data)
    ids = insert_vectors(collection_name, vectors, set_type, class_labels, metadata=metadata)
    total_inserted = sum(batch.insert_count for batch in ids)
    st.write(f"âœ… {set_type} data (size: {total_inserted}) successfully inserted into {collection_name} collection.")
    return ids

#  --------------------------------------------- Main ---------------------------------------------
def render():
    """Vector Database í˜ì´ì§€ ë Œë”ë§"""
    if 'dataset_name' not in st.session_state:
        st.error("ë°ì´í„°ì…‹ ì´ë¦„ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Upload Data íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    dataset_name = st.session_state['dataset_name']
    
    # ì´ì „ ì„¸ì…˜ ë°ì´í„° ë³µì› ì‹œë„
    if 'metadata_restored' not in st.session_state:
        st.info("ì´ì „ ì„¸ì…˜ ë°ì´í„° í™•ì¸ ì¤‘...")
        restored_data = load_metadata_from_vectordb(dataset_name)
        if restored_data:
            st.session_state['metadata_restored'] = True
            st.success("âœ… ì´ì „ ì„¸ì…˜ ë°ì´í„°ê°€ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.session_state['metadata_restored'] = False

    # ë°ì´í„° ë¡œë“œ
    train_df, valid_df, test_df = get_data_from_session()
    if train_df is None or valid_df is None or test_df is None:
        st.error("ë°ì´í„°ì…‹ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Upload Data íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # ë°ì´í„° ì„ë² ë”© ë° ì €ì¥
    text_col, class_col = split_columns(train_df)
    
    if not text_col or not class_col:
        st.error("ë°ì´í„°ì…‹ì— ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë˜ëŠ” í´ë˜ìŠ¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    embedding_pipeline = EmbeddingPipeline()
    embedding_pipeline.load_model()
    
    collection_name = dataset_name
    metadata = prepare_metadata()
    
    # ë°ì´í„° ì„ë² ë”© ë° ì €ì¥
    with st.spinner("ë°ì´í„° ì„ë² ë”© ì¤‘..."):
        # Train ë°ì´í„° (ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜)
        train_embeddings = embedding_pipeline.generate_embeddings(train_df, text_col)
        train_class_labels = train_df[class_col[0]].squeeze().tolist()
        load_and_save_data(train_embeddings, collection_name, "train", train_class_labels, metadata=metadata)
        
        # Validation ë°ì´í„°
        valid_embeddings = embedding_pipeline.generate_embeddings(valid_df, text_col)
        valid_class_labels = valid_df[class_col[0]].squeeze().tolist()
        load_and_save_data(valid_embeddings, collection_name, "valid", valid_class_labels)
        
        # Test ë°ì´í„°
        test_embeddings = embedding_pipeline.generate_embeddings(test_df, text_col)
        test_class_labels = test_df[class_col[0]].squeeze().tolist()
        load_and_save_data(test_embeddings, collection_name, "test", test_class_labels)
    
    # ê²°ê³¼ í™•ì¸
    collection = Collection(name=collection_name)
    collection.load()
    
    # ê° set_typeë³„ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
    for set_type in ['train', 'valid', 'test', 'metadata']:
        results = collection.query(expr=f"set_type == '{set_type}'", output_fields=["set_type"], limit=10000)
        #st.write(f"ğŸ” {set_type.capitalize()} ë ˆì½”ë“œ: {len(results)}ê°œ")
    
    st.success("âœ… ëª¨ë“  ë°ì´í„°ì…‹ì´ Vector Databaseì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì „ì²´ ë°ì´í„° íƒ€ì… ìš”ì•½
    results = collection.query(
        expr="set_type in ['train', 'valid', 'test', 'metadata']", 
        output_fields=["set_type"], 
        limit=10000
    )
    set_types = [res["set_type"] for res in results]
    unique_types = set(set_types)
    st.write("ğŸ“Š ì»¬ë ‰ì…˜ì— ì €ì¥ëœ ë°ì´í„° íƒ€ì…:", unique_types)
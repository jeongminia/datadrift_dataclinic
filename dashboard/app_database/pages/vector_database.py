import streamlit as st
from pymilvus import utility, Collection, CollectionSchema, FieldSchema, DataType, connections
import numpy as np
import json
import os
import time

# Import utils from parent directory
try:
    from ..utils import EmbeddingPipeline, split_columns, get_data_from_session
except ImportError:
    # Fallback for standalone execution
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from utils import EmbeddingPipeline, split_columns, get_data_from_session

# Milvus ì„œë²„ì— ì—°ê²°
connections.connect("default", host="localhost", port="19530")

def create_collection(collection_name):
    """í†µí•© ì»¬ë ‰ì…˜ ìƒì„± (ë°ì´í„° + ë©”íƒ€ë°ì´í„°)"""
    if not utility.has_collection(collection_name):
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="set_type", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="class", dtype=DataType.VARCHAR, max_length=50), 
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=768),
            # ë©”íƒ€ë°ì´í„° í•„ë“œë“¤
            FieldSchema(name="dataset_name", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="summary_dict", dtype=DataType.VARCHAR, max_length=5000),
            FieldSchema(name="data_previews", dtype=DataType.VARCHAR, max_length=10000),
            FieldSchema(name="class_dist_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="doc_len_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="doc_len_table", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="wordcloud_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="timestamp", dtype=DataType.INT64)
        ]
        schema = CollectionSchema(fields=fields, description="Text Embeddings with Metadata")
        collection = Collection(name=collection_name, schema=schema)
        collection.create_index(
            field_name="vector", 
            index_params={"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": 128}}
        )
        st.write(f"âœ… Created collection: {collection_name}")
    else:
        collection = Collection(name=collection_name)
    collection.load()
    return collection

def make_json_serializable(obj):
    """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    import pandas as pd
    
    try:
        if obj is None:
            return None
        elif isinstance(obj, (str, int, float, bool)):
            return obj
        elif isinstance(obj, dict):
            return {k: make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [make_json_serializable(item) for item in obj]
        elif isinstance(obj, pd.DataFrame):
            return obj.to_dict('records')
        elif isinstance(obj, pd.Series):
            return obj.tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.generic):
            return obj.item()
        elif hasattr(obj, 'dtype') and 'object' in str(obj.dtype):
            return str(obj)
        elif hasattr(obj, 'to_dict'):
            return obj.to_dict()
        elif hasattr(obj, 'tolist'):
            return obj.tolist()
        else:
            return str(obj)
    except Exception:
        return str(obj)

def prepare_metadata():
    """ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•˜ê³  ì§ë ¬í™”"""
    try:
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        summary_dict = st.session_state.get("dataset_summary", {})
        class_dist_path = st.session_state.get("class_dist_path", "")
        doc_len_path = st.session_state.get("doc_len_path", "")
        doc_len_table = st.session_state.get("doc_len_table", "")
        wordcloud_path = st.session_state.get("wordcloud_path", "")
        dataset_name = st.session_state.get("dataset_name", "")
        
        train_df, valid_df, test_df = get_data_from_session()
        data_previews = {}
        
        if train_df is not None and len(train_df) > 0:
            # ìƒìœ„ 10ê°œ í–‰ë§Œ ì €ì¥ + ì „ì²´ shape ì •ë³´
            preview_data = train_df.head(5).to_dict('records')
            data_previews["train"] = {
                "data": preview_data,
                "total_rows": len(train_df),
                "columns": list(train_df.columns)
            }
        
        if valid_df is not None and len(valid_df) > 0:
            preview_data = valid_df.head(5).to_dict('records')
            data_previews["valid"] = {
                "data": preview_data,
                "total_rows": len(valid_df),
                "columns": list(valid_df.columns)
            }
            
        if test_df is not None and len(test_df) > 0:
            preview_data = test_df.head(5).to_dict('records')
            data_previews["test"] = {
                "data": preview_data,
                "total_rows": len(test_df),
                "columns": list(test_df.columns)
            }
        
        # ê²½ë¡œë“¤ì„ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
        paths = {
            "class_dist_path": os.path.abspath(class_dist_path) if class_dist_path else "",
            "doc_len_path": os.path.abspath(doc_len_path) if doc_len_path else "",
            "wordcloud_path": os.path.abspath(wordcloud_path) if wordcloud_path else ""
        }
        
        # ì•ˆì „í•œ JSON ì§ë ¬í™”
        summary_dict_json = ""
        doc_len_table_json = ""
        data_previews_json = ""
        
        if summary_dict:
            try:
                summary_dict_json = json.dumps(make_json_serializable(summary_dict))
            except Exception:
                summary_dict_json = ""
        
        if doc_len_table:
            try:
                doc_len_table_json = json.dumps(make_json_serializable(doc_len_table))
            except Exception:
                doc_len_table_json = ""
        
        # ğŸ”¥ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° JSON ì§ë ¬í™” (ì‘ì€ ìš©ëŸ‰)
        if data_previews:
            try:
                data_previews_json = json.dumps(make_json_serializable(data_previews))
            except Exception:
                data_previews_json = ""
        
        return {
            "dataset_name": dataset_name,
            "summary_dict": summary_dict_json,
            "data_previews": data_previews_json,  # ğŸ”¥ ì¶”ê°€
            "class_dist_path": paths["class_dist_path"],
            "doc_len_path": paths["doc_len_path"],
            "doc_len_table": doc_len_table_json,
            "wordcloud_path": paths["wordcloud_path"],
            "timestamp": int(time.time())
        }
        
    except Exception as e:
        st.error(f"ë©”íƒ€ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "dataset_name": st.session_state.get("dataset_name", ""),
            "summary_dict": "",
            "data_previews": "",  # ğŸ”¥ ì¶”ê°€
            "class_dist_path": "",
            "doc_len_path": "",
            "doc_len_table": "",
            "wordcloud_path": "",
            "timestamp": int(time.time())
        }

def save_metadata_to_vectordb(collection_name):
    """ë©”íƒ€ë°ì´í„°ë¥¼ ë©”ì¸ ì»¬ë ‰ì…˜ì— ì €ì¥"""
    try:
        collection = Collection(name=collection_name)
        collection.load()
        
        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì‚­ì œ
        metadata_results = collection.query(
            expr="set_type == 'metadata'",
            output_fields=["id"],
            limit=1
        )
        
        if metadata_results:
            metadata_id = metadata_results[0]["id"]
            collection.delete(f"id == {metadata_id}")
        
        # ìƒˆ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        metadata = prepare_metadata()
        dummy_vector = [0.0] * 768
        
        # ë©”íƒ€ë°ì´í„° ì‚½ì… (data_previews í•„ë“œ ì¶”ê°€)
        data = [
            ["metadata"],
            ["metadata"],
            [dummy_vector],
            [metadata["dataset_name"]],
            [metadata["summary_dict"]],
            [metadata["data_previews"]],  # ğŸ”¥ ì¶”ê°€
            [metadata["class_dist_path"]],
            [metadata["doc_len_path"]],
            [metadata["doc_len_table"]],
            [metadata["wordcloud_path"]],
            [metadata["timestamp"]]
        ]
        
        fields = ["set_type", "class", "vector", "dataset_name", "summary_dict", 
                 "data_previews", "class_dist_path", "doc_len_path", "doc_len_table", "wordcloud_path", "timestamp"]
        
        result = collection.insert(data, fields=fields)
        collection.flush()
        
        st.success(f"âœ… ë©”íƒ€ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return result.primary_keys[0]
        
    except Exception as e:
        st.error(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def load_metadata_from_vectordb(collection_name):
    """ë©”ì¸ ì»¬ë ‰ì…˜ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
    try:
        if not utility.has_collection(collection_name):
            return None
        
        collection = Collection(name=collection_name)
        collection.load()
        
        # ë©”íƒ€ë°ì´í„° ì¡°íšŒ (data_previews í•„ë“œ ì¶”ê°€)
        results = collection.query(
            expr="set_type == 'metadata'",
            output_fields=["dataset_name", "summary_dict", "data_previews", "class_dist_path", 
                          "doc_len_path", "doc_len_table", "wordcloud_path", "timestamp"],
            limit=1
        )
        
        if not results:
            return None
            
        result = results[0]
        
        # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ë¡œë“œ
        st.session_state["dataset_name"] = result["dataset_name"]
        st.session_state["class_dist_path"] = result["class_dist_path"]
        st.session_state["doc_len_path"] = result["doc_len_path"]
        st.session_state["wordcloud_path"] = result["wordcloud_path"]
        
        # JSON íŒŒì‹±
        if result["summary_dict"]:
            try:
                st.session_state["dataset_summary"] = json.loads(result["summary_dict"])
            except:
                st.session_state["dataset_summary"] = {}
        
        # data_previews íŒŒì‹± (JSON í˜•íƒœë¡œ ì €ì¥ë¨)
        if result.get("data_previews"):
            try:
                st.session_state["data_previews"] = json.loads(result["data_previews"])
            except:
                st.session_state["data_previews"] = {}
        else:
            st.session_state["data_previews"] = {}
        
        if result["doc_len_table"]:
            try:
                st.session_state["doc_len_table"] = json.loads(result["doc_len_table"])
            except:
                st.session_state["doc_len_table"] = result["doc_len_table"]
        else:
            st.session_state["doc_len_table"] = ""
        
        return result
        
    except Exception as e:
        st.error(f"âŒ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def insert_vectors(collection_name, vectors, set_type, class_labels, batch_size=500, metadata=None):
    """ë²¡í„° ë°ì´í„°ë¥¼ ì»¬ë ‰ì…˜ì— ì‚½ì…"""
    collection = Collection(name=collection_name)
    class_labels = [str(label) if not isinstance(label, str) else label for label in class_labels]
    total = len(vectors)
    ids = []
    
    # ë©”íƒ€ë°ì´í„° ì‚½ì… (train ë°ì´í„°ì™€ í•¨ê»˜ ì²˜ë¦¬)
    if metadata and set_type == "train":
        dummy_vector = [0.0] * 768
        
        metadata_data = [
            ["metadata"],
            ["metadata"],
            [dummy_vector],
            [metadata["dataset_name"]],
            [metadata["summary_dict"]],
            [metadata["data_previews"]],  # ğŸ”¥ ì¶”ê°€
            [metadata["class_dist_path"]],
            [metadata["doc_len_path"]],
            [metadata["doc_len_table"]],
            [metadata["wordcloud_path"]],
            [metadata["timestamp"]]
        ]
        
        fields = ["set_type", "class", "vector", "dataset_name", "summary_dict", 
                 "data_previews", "class_dist_path", "doc_len_path", "doc_len_table", "wordcloud_path", "timestamp"]
        
        metadata_ids = collection.insert(metadata_data, fields=fields)
        ids.append(metadata_ids)
        collection.flush()
    
    # ì¼ë°˜ ë°ì´í„° ì‚½ì…
    for i in range(0, total, batch_size):
        batch_vectors = vectors[i:i+batch_size]
        batch_labels = class_labels[i:i+batch_size]
        batch_set_type = [set_type] * len(batch_vectors)
        
        # ë©”íƒ€ë°ì´í„° í•„ë“œë“¤ì€ ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •
        empty_metadata = [""] * len(batch_vectors)
        zero_timestamp = [0] * len(batch_vectors)
        
        data = [
            batch_set_type,
            batch_labels,
            batch_vectors,
            empty_metadata,  # dataset_name
            empty_metadata,  # summary_dict
            empty_metadata,  # data_previews ğŸ”¥ ì¶”ê°€
            empty_metadata,  # class_dist_path
            empty_metadata,  # doc_len_path
            empty_metadata,  # doc_len_table
            empty_metadata,  # wordcloud_path
            zero_timestamp   # timestamp
        ]
        
        fields = ["set_type", "class", "vector", "dataset_name", "summary_dict", 
                 "data_previews", "class_dist_path", "doc_len_path", "doc_len_table", "wordcloud_path", "timestamp"]
        
        batch_ids = collection.insert(data, fields=fields)
        ids.append(batch_ids)
        collection.flush()
    
    return ids

def load_and_save_data(data, collection_name, set_type, class_labels, metadata=None):
    """ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë²¡í„°DBì— ì €ì¥"""
    create_collection(collection_name)
    vectors = np.array(data)
    ids = insert_vectors(collection_name, vectors, set_type, class_labels, metadata=metadata)
    total_inserted = sum(batch.insert_count for batch in ids)
    st.write(f"âœ… {set_type} data (size: {total_inserted}) successfully inserted into {collection_name} collection.")
    return ids

# ...existing code...

def render():
    """Vector Database í˜ì´ì§€ ë Œë”ë§"""
    if 'dataset_name' not in st.session_state:
        st.error("ë°ì´í„°ì…‹ ì´ë¦„ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Upload Data íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    dataset_name = st.session_state['dataset_name']
    
    # ì´ì „ ì„¸ì…˜ ë°ì´í„° ë³µì› ì‹œë„
    if 'metadata_restored' not in st.session_state:
        st.info("ì´ì „ ì„¸ì…˜ ë°ì´í„° í™•ì¸ ì¤‘...")
        restored_data = load_metadata_from_vectordb(dataset_name)
        if restored_data:
            st.session_state['metadata_restored'] = True
            st.success("âœ… ì´ì „ ì„¸ì…˜ ë°ì´í„°ê°€ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.session_state['metadata_restored'] = False

    # ë°ì´í„° ë¡œë“œ
    train_df, valid_df, test_df = get_data_from_session()
    if train_df is None or valid_df is None or test_df is None:
        st.error("ë°ì´í„°ì…‹ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Upload Data íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # ë°ì´í„° ì„ë² ë”© ë° ì €ì¥
    st.subheader("Data Embedding")
    text_col, class_col = split_columns(train_df)
    
    if not text_col or not class_col:
        st.error("ë°ì´í„°ì…‹ì— ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë˜ëŠ” í´ë˜ìŠ¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    embedding_pipeline = EmbeddingPipeline()
    embedding_pipeline.load_model()
    
    collection_name = dataset_name
    metadata = prepare_metadata()
    
    # ë°ì´í„° ì„ë² ë”© ë° ì €ì¥
    with st.spinner("ë°ì´í„° ì„ë² ë”© ì¤‘..."):
        # Train ë°ì´í„° (ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜)
        train_embeddings = embedding_pipeline.generate_embeddings(train_df, text_col)
        train_class_labels = train_df[class_col[0]].squeeze().tolist()
        load_and_save_data(train_embeddings, collection_name, "train", train_class_labels, metadata=metadata)
        
        # Validation ë°ì´í„°
        valid_embeddings = embedding_pipeline.generate_embeddings(valid_df, text_col)
        valid_class_labels = valid_df[class_col[0]].squeeze().tolist()
        load_and_save_data(valid_embeddings, collection_name, "valid", valid_class_labels)
        
        # Test ë°ì´í„°
        test_embeddings = embedding_pipeline.generate_embeddings(test_df, text_col)
        test_class_labels = test_df[class_col[0]].squeeze().tolist()
        load_and_save_data(test_embeddings, collection_name, "test", test_class_labels)
    
    # ê²°ê³¼ í™•ì¸
    collection = Collection(name=collection_name)
    collection.load()
    
    # ê° set_typeë³„ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
    for set_type in ['train', 'valid', 'test', 'metadata']:
        results = collection.query(expr=f"set_type == '{set_type}'", output_fields=["set_type"], limit=10000)
        #st.write(f"ğŸ” {set_type.capitalize()} ë ˆì½”ë“œ: {len(results)}ê°œ")
    
    st.success("âœ… ëª¨ë“  ë°ì´í„°ì…‹ì´ Vector Databaseì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì „ì²´ ë°ì´í„° íƒ€ì… ìš”ì•½
    results = collection.query(
        expr="set_type in ['train', 'valid', 'test', 'metadata']", 
        output_fields=["set_type"], 
        limit=10000
    )
    set_types = [res["set_type"] for res in results]
    unique_types = set(set_types)
    st.write("ğŸ“Š ì»¬ë ‰ì…˜ì— ì €ì¥ëœ ë°ì´í„° íƒ€ì…:", unique_types)